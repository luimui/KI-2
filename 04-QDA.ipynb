{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luimui/KI-2/blob/main/04-QDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2TnjWEDi7Tt"
      },
      "source": [
        "# Linear and quadratic discriminant analysis\n",
        "In todays lecture we will cover linear (LDA) and quadratic discriminant analysis (QDA) and how we can use them for classification.\n",
        "But before we dive in deeper think about what we actually trying to achieve with this approaches.\n",
        "\n",
        "We have learned that we use so-called discriminants to represent classifiers. For a classifier with $\\omega_1,...,\\omega_n$ classes, we have a set of discriminant functions $g_i(\\mathbf{x})$ with $i \\in \\{1,...,c\\}$.    \n",
        "*Task*: Give some examples for discriminants with a minimal error rate!    \n",
        "* $g_i(x) = P(ω_i|x) =  p(x|ω_i)P(ω_i)p(x) $\n",
        "<p>\n",
        " $ g_i(x) = p(x|ω_i)P(ω_i) $ geht auch weil p(x) fix ist\n",
        " <p>  \n",
        "  $g_i(x) = ln p(x|ω_i) + ln P(ω_i)$\n",
        "\n",
        "  <br>\n",
        "  Richig: $P(\\omega_i|\\mathbf{x}) = \\frac{P(x \\mid \\omega_i) * P(\\omega_i)}{P(x)}$ hier kann p(x) weggelassen werden weil fix\n",
        "  <br>\n",
        "  Case: all independent features: $\\sum = \\sigma^2 * I$\n",
        "  <p>\n",
        "  $g(x_i) = 1/σ^2 * μ_i + −1/σ^2 *μ^t_i μ_i + ln P(ω_i)$         \n",
        "\n",
        "*Task*: What happens with the feature space $\\mathbf{R}^d$ when we use such discriminants/decission rules?\n",
        "* It cuts the feature space  $\\mathbf{R}^d$ into $R_1 ... R_c$ decision regions        \n",
        "\n",
        "*Task*: How do we decide to which class $\\omega_i$ a feature vetor $\\mathbf{x}$ belongs?\n",
        "* By highest a priori probability of feature $x$ given class $w_i$ $p(x \\mid w_1)$ or the lowest error $1-p(x \\mid w_1)$\n",
        "<p>\n",
        "Richtig: compute decision planes / boundaries\n",
        "<p>\n",
        "decision booundary is definded as the area where the two biggesst discriminants have the same value        \n",
        "<p>\n",
        "x belogns to the class w_i if g_i(x) > g_j(x) j!=i\n",
        "<br>\n",
        "\n",
        "Now that we have an idea about discriminants, we can dive deeper into our LDA and QDA classifiers.    \n",
        "*Task*: We want to calculate the probability that a feature vector $\\mathbf{x}$ belongs to a class $\\mathbf{\\omega_i}$. What can we use to calculate $P(\\omega_i|\\mathbf{x})$?    \n",
        "* $P(\\omega_i|\\mathbf{x}) = \\frac{P(x \\mid \\omega_i) * P(\\omega_i)}{P(x)}$     \n",
        "\n",
        "*Task*: What kind of assumption do we have to make when we use an LDA/QDA?\n",
        "* Both: normal distributed instances of classes, univariate (one feature): sigma^2 is a single value, multivariate(more features): sigma^2 is a covariance matrix    \n",
        "<p>\n",
        "LDA: independent features or all classes have the same shape of normal dsitribution --> linear decision boundaries, sigma^2 is calculated over all classes as mean\n",
        "<p>\n",
        "QDA: classes are normal distributed but do not have the same shape and size --> quadratic decision boundaries, sigma^2 si calculated for every class\n",
        "\n",
        "*Task*: What is the difference between a LDA and QDA?\n",
        "* Since the QDA has to determine significantly more parameters than the LDA,\n",
        "the uncertainty (variance) of the estimate is significantly higher for the same\n",
        "number of training data    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le_VF9zhi7Tu"
      },
      "source": [
        "## Exercise 1\n",
        "A fisherman needs your help in classifying fish. He recently caught the following fish:\n",
        "\n",
        "| Length (m)    | Species          |\n",
        "| ------------- |-------------  |\n",
        "| 1.3           | Seabass       |\n",
        "| 0.7           | Salmon       |\n",
        "| 0.62           | Salmon      |\n",
        "| 0.9           | Salmon       |\n",
        "| 0.91          | Seabass       |\n",
        "| 0.31          | Herring       |\n",
        "| 0.26           | Herring       |\n",
        "\n",
        "* Calculate the priors $p(\\omega_i)$ for each fish species\n",
        "* What is the formula for calculating the parameters $\\mu$ and $\\sigma^2$ for the likelihoods?\n",
        "* Calculate the parameters $\\mu$ and $\\sigma^2$ for the likelihoods $p(\\mathbf{x}|\\omega_i)$.\n",
        "* The fisherman catches a new fish with length $x = 0.82 m$. Calculate the posterior probability $p(\\omega_i|\\mathbf{x})$ for each class. How is the fish classified?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_w_sb = 2/7\n",
        "p_w_sa = 3/7\n",
        "p_w_h = 2/7"
      ],
      "metadata": {
        "id": "imVLHjMe0AvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "u = 1/n * sum(x_i)\n",
        "sigma_2 = 1/n * sum((x_i-u)^2)"
      ],
      "metadata": {
        "id": "x6SzNeLA1fo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "u_sb = 1/2 * (1.3 + 0.91)\n",
        "sigma_2_sb = 1/1 * ((1.3-u_sb)**2 + (0.91-u_sb)**2)\n",
        "sigma_sb = math.sqrt(sigma_2_sb)\n",
        "\n",
        "u_sa = 1/3 * (0.7 + 0.62 + 0.9)\n",
        "sigma_2_sa = 1/2 * ((0.7-u_sa)**2 + (0.62-u_sa)**2 + (0.9-u_sa)**2)\n",
        "sigma_sa = math.sqrt(sigma_2_sa)\n",
        "\n",
        "u_h = 1/2 * (0.31 + 0.26)\n",
        "sigma_2_h = 1/1 * ((0.31-u_h)**2 + (0.26-u_h)**2)\n",
        "sigma_h = math.sqrt(sigma_2_h)\n",
        "\n",
        "print(\"This is QDA, variance is calculated for EVERY class\")\n",
        "print(\"these are the likelihoods\")\n",
        "print(\"u_sb: \" + str(u_sb) + \" sigma_2_sb: \" + str(sigma_2_sb) + \" sigma_sb: \" + str(sigma_sb))\n",
        "print(\"u_sa: \" + str(u_sa) + \" sigma_2_sa: \" + str(sigma_2_sa) + \" sigma_sa: \" + str(sigma_sa))\n",
        "print(\"u_h: \" + str(u_h) + \" sigma_2_h: \" + str(sigma_2_h) + \" sigma_h: \" + str(sigma_h))"
      ],
      "metadata": {
        "id": "qBLCCmue1eKl",
        "outputId": "78a7ddef-97a6-4cbf-faf9-1b61653a0b83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is QDA, variance is calculated for EVERY class\n",
            "these are the likelihoods\n",
            "u_sb: 1.105 sigma_2_sb: 0.07605 sigma_sb: 0.27577164466275356\n",
            "u_sa: 0.7399999999999999 sigma_2_sa: 0.020800000000000006 sigma_sa: 0.1442220510185596\n",
            "u_h: 0.28500000000000003 sigma_2_h: 0.0012499999999999994 sigma_h: 0.03535533905932737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#richtig posterior:\n",
        "# x=08.2 in Normalverteilung einsetzen = p(x|w_i), diesen wert mal prior wahrscheinlichkeit = 2/7 multiplizieren"
      ],
      "metadata": {
        "id": "Y6u74c0CNQyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "z=((0.82 - u_sb)/sigma_sb)\n",
        "p=(1/(sigma_sb*math.sqrt(2*math.pi)) * math.pow(math.e,  -0.5*math.pow(z,2)) )\n",
        "print(\"z: \" + str(z) + \" p(x|class sb): \" + str(p) + \" p(p_sb|x): \" + str(p*p_w_sb))\n",
        "\n",
        "z=((0.82 - u_sa)/sigma_sa)\n",
        "p=(1/(sigma_sa*math.sqrt(2*math.pi)) * math.pow(math.e, -0.5*math.pow(z,2)) )\n",
        "print(\"z: \" + str(z) + \" p: \" + str(p) + \" p(u_sa|x): \" + str(p*p_w_sa))\n",
        "\n",
        "z=((0.82 - u_h)/sigma_h)\n",
        "p=(1/(sigma_h*math.sqrt(2*math.pi)) * math.pow(math.e, -0.5*math.pow(z,2)) )\n",
        "print(\"z: \" + str(z) + \" p: \" + str(p) + \" p(u_h|x): \" + str(p*p_w_h))\n"
      ],
      "metadata": {
        "id": "caAtwpwa2rMY",
        "outputId": "7313bc85-1a61-4ef0-9fe9-c2b5bcd3e5f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z: -1.0334637571188003 p: 0.8480802004114931 p(u_sb|x): 0.24230862868899802\n",
            "z: 0.5547001962252296 p: 2.371722439940712 p(u_sa|x): 1.0164524742603052\n",
            "z: 15.132085117392117 p: 2.1383550972682576e-49 p(u_h|x): 6.109585992195021e-50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaoMHxxoi7Tv"
      },
      "source": [
        "## Exercise 2\n",
        "Implement a function `priors(classes)` that outputs the prior $p(\\omega)$ for each class for a vector of class labels.\n",
        "The input should be an array of classes (e.g. `np.array([\"stand\", \"sit\", \"sit\", \"stand\"])`). The output should be a data frame with the columns `class` and `prior`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm_Teu8ii7Tv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def priors(classes):\n",
        "    'Implement me!'\n",
        "\n",
        "pp = priors(np.array([\"stand\",\"sit\",\"sit\",\"sit\",\"stand\"]))\n",
        "print(pp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZEISXsPi7Tv"
      },
      "source": [
        "## Exercise 3\n",
        "Implement a function `likelihood(data)` that approximates the likelihood $p(\\omega_i|\\mathbf{x})$ for each class $\\omega_i$ with a normal distribution for a data frame consisting of a column $y$ and a column $x$, i.e. a mean value and a variance are to be output for each class.\n",
        "The output should therefore have the columns `class`, `mean` and `variance`.\n",
        "\n",
        "Plot the likelihood for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOWtYNDwi7Tw"
      },
      "outputs": [],
      "source": [
        "from scipy.io import arff\n",
        "\n",
        "def likelihood(data):\n",
        "    'Implement me!'\n",
        "\n",
        "data = arff.loadarff('features1.arff')\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "dat = df.loc[:, [\"AccX_mean\",\"class\"]]\n",
        "dat.columns = [\"x\",\"class\"]\n",
        "lik = likelihood(dat)\n",
        "lik"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOyRROjri7Tw"
      },
      "source": [
        "## Exercise 4\n",
        "Implement a function myqda(newdat,lik,priors) that returns the most probable class for a new observation `newdat`.\n",
        "\n",
        "Test your implementation on the dataset `features1.arff`. \"Train\" the QDA (i.e. calculate likelihood and prior), and then perform classification on the same data. How good is the classification?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sxDPW6_i7Tw"
      },
      "outputs": [],
      "source": [
        "from scipy.io import arff\n",
        "import scipy.stats\n",
        "\n",
        "def myqda(newdat,lik,prior):\n",
        "    'Implement me!'\n",
        "\n",
        "\n",
        "data = arff.loadarff('features1.arff')\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "dat = df.loc[:, [\"AccX_mean\",\"class\"]]\n",
        "dat.columns = [\"x\",\"class\"]\n",
        "\n",
        "lik = likelihood(dat)\n",
        "prior = priors(dat[\"class\"])\n",
        "\n",
        "nc = myqda(dat[\"x\"][1:100],lik,prior)\n",
        "print(sum(nc == dat[\"class\"][1:100])/100) # compute fraction of correct classified data points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tjRAHcRi7Tw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}