{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luimui/KI-2/blob/main/03-neural-networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ClvnR4gfid"
      },
      "source": [
        "# Neural networks\n",
        "\n",
        "In this exercise, we will look at two very simple examples of neural networks. If you are interested in neural networks beyond that, I recommend the course \"AI 3: Artificial Neural Networks\" in the summer semester.\n",
        "\n",
        "If you are interested in the theory and mathematical details, I highly recommend the following book: https://www.deeplearningbook.org/ (by Yoshua Bengio, who, together with Geoffrey Hinton and Yann LeCun, received the 2018 Turing Award for work on deep learning methods).\n",
        "\n",
        "*Task 1*: Explain the core idea and structure of neurons and neural networks!\n",
        "* Slides Intermediate defense\n",
        "* see MA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QFKAE0mgfie"
      },
      "source": [
        "## Neural networks for regression\n",
        "\n",
        "In this exercise we use `PyTorch` for training neural networks. You can find more information about this package here: https://pytorch.org\n",
        "\n",
        "However, the examples we look at in the exercise should (hopefully) be fairly self-explanatory. In the example below, we describe the network structure in PyTorch. This network consists of only a single linear layer with only one node, the activation function is the identity. We also specify that we want to use the mean squared error as the loss function and the Adam optimizer as the optimizer.\n",
        "   \n",
        "To define a neural network, we now need to write a class that inherits from the `torch.Module` class. We have to provide an `__init__()` and `forward()` method. In the `__init__()` method, we define the structure of our neural network. The `forward()` method defines how the so-called forward pass is calculated for our neural network (i.e. how the NN maps the input to the output).    \n",
        "\n",
        "*Task*: Get familiar with code below as it serves as a basis for the upcoming tasks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yqoqz1Xogfie"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    \"\"\"Class for our linear model. Here we have to overwrite some of the generic funcionalities of the nn.Module class of PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"This is basically how we define our model architecture.\n",
        "        This function will be automatically called when we initialize an instance of our LinearModel class.\n",
        "        \"\"\"\n",
        "        # execute initialization code for nn.Module()\n",
        "        super(LinearModel, self).__init__()\n",
        "        # Building block for our network architecture -> a linear layer\n",
        "        self.linear = nn.Linear(1,          # number of inputs into the layer\n",
        "                                1,          # number of units in the layer\n",
        "                                bias=True)  # nn.linear() has a bias term by default\n",
        "\n",
        "        # Activation functions we want to use for our network\n",
        "        self.identity = nn.Identity()       # Identity function\n",
        "\n",
        "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
        "        \"\"\"With this method we define how the forward pass of the model is computed.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input to our network.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output of the network.\n",
        "        \"\"\"\n",
        "        x = self.linear(x)      # feed input to linear layer\n",
        "        x = self.identity(x)    # compute identity on the ouput of our linear layer\n",
        "\n",
        "        return x\n",
        "\n",
        "# initialize the model\n",
        "model = LinearModel()\n",
        "\n",
        "# define the loss function and optimizer for our network\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1) # Adam optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGqd3d65gfif"
      },
      "source": [
        "In order to be able to train and evaluate our model, we need to also specify the respective function.    \n",
        "\n",
        "*Task*: Get familiar with code below as it serves as a basis for the upcoming tasks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HLkuYtr-gfif"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def train_linear_model(model: nn.Module, X: torch.Tensor, y:torch.Tensor, criterion:Callable, optimizer: Callable):\n",
        "    \"\"\"This is the function with which we can train our model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model we want to train.\n",
        "        X (torch.Tensor): The input we want to use to train our model.\n",
        "        y (torch.Tensor): The targets which belong to the inputs in X.\n",
        "        criterion (Callable): Our loss function.\n",
        "        optimizer (Callable): The optimizer we want to use to train our model.\n",
        "    \"\"\"\n",
        "    # compute predictions for all inputs\n",
        "    y_pred = model(X)\n",
        "    # compute average loss over all precictions\n",
        "    loss = criterion(y_pred, y)\n",
        "    # reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "    # compute the gradients\n",
        "    loss.backward()\n",
        "    # optimize the network using the computet gradients\n",
        "    optimizer.step()\n",
        "\n",
        "def evaluate_linear_model(model: nn.Module, X: torch.Tensor, y: torch.Tensor, criterion: Callable) -> float:\n",
        "    \"\"\"This is the function with which we can evaluate our model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model we want to train.\n",
        "        X (torch.Tensor): The input we want to use to train our model.\n",
        "        y (torch.Tensor): The targets which belong to the inputs in X.\n",
        "        criterion (Callable): Our loss function.\n",
        "\n",
        "    Returns:\n",
        "        float: The average loss for the test dataset.\n",
        "    \"\"\"\n",
        "    # compute no gradients for all inputs -> better performance\n",
        "    with torch.no_grad():\n",
        "        # compute predictions for all inputs\n",
        "        y_pred = model(X)\n",
        "        # compute average loss over all precictions\n",
        "        l = criterion(y_pred, y).item()\n",
        "        return l\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVPUOlrcgfig"
      },
      "source": [
        "Now we have everything we need to train our first neural network. In the example below, we repeat the training over 5 iterations, training 25 epochs in each iteration (this means with each iteration we go through the complete training dataset 25 times).    \n",
        "\n",
        "*Task*: Execute the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoVBgq9ogfig",
        "outputId": "e16ebb40-e831-400a-e51e-9b95fc37e7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 1/5: 100%|██████████| 25/25 [00:00<00:00, 1204.22epoch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 25 epochs: 2.7031302452087402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 2/5: 100%|██████████| 25/25 [00:00<00:00, 1075.73epoch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 50 epochs: 2.7176971435546875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 3/5: 100%|██████████| 25/25 [00:00<00:00, 1234.46epoch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 75 epochs: 2.7248427867889404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 4/5: 100%|██████████| 25/25 [00:00<00:00, 957.15epoch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 100 epochs: 2.7327001094818115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 5/5: 100%|██████████| 25/25 [00:00<00:00, 1179.12epoch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 125 epochs: 2.738478183746338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# load the iris dataset into a dataframe\n",
        "df = pd.read_csv('/content/drive/MyDrive/KI2WS202324/iris.csv')\n",
        "\n",
        "\n",
        "# get features X and targets y and convert them into tensors\n",
        "X = torch.tensor(df.SepalWidth.values, dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor(df.PetalLength.values, dtype=torch.float32).view(-1, 1)\n",
        "# split X and y into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
        "\n",
        "# set the number of epochs and iterations\n",
        "epochs = 25\n",
        "iterations = 5\n",
        "num_epochs = 0\n",
        "\n",
        "# execute the training and evluation\n",
        "for i in range(iterations):\n",
        "    with tqdm(total=epochs, desc=f'Iteration {i+1}/{5}', unit='epoch') as pbar:\n",
        "        # training of the neural network\n",
        "        for epoch in range (epochs):\n",
        "            train_linear_model(model, X_train, y_train, criterion, optimizer)\n",
        "            pbar.update(1)\n",
        "        # evaluation of the neural network\n",
        "        test_loss = evaluate_linear_model(model, X_test, y_test, criterion)\n",
        "        num_epochs = num_epochs+epochs\n",
        "        print(f'\\nLoss after {num_epochs} epochs: {test_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X;"
      ],
      "metadata": {
        "id": "tIlT5yM2oEJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDZkOvcRgfig"
      },
      "source": [
        "*Task*: What does our input $X$ look like?\n",
        "* $X = $ Tensor / like np.array of floats from 2 to 4  / n*1-matix  \n",
        "\n",
        "*Task*: What kind of function does this network represent?   \n",
        "* $\\hat{y} = \\phi(\\beta_0 + beta_1*x_1)$  $\\phi is the identity$ linear function, as the input is intercept + one feature in design matrix and activation funciton is just identity (instead of sigmoid function to squish into [0,1]\n",
        "\n",
        "*Task*: What happens if the input is of the form $x = [x_1, x_2, ..., x_n]^T$? What does our weight matrix $W$ look like? What is the equation for $\\hat{y}$?\n",
        "* $W = 1 * n -Vector$\n",
        "* $\\hat{y} = f(\\beta_0 + \\beta_1*x_1 + ... \\beta_n*x_n)$\n",
        "\n",
        "*Task*: What happens if we additionally extend our layer to k units/neurons? What does our weight matrix $W$ look like? What is the equation for $\\hat{y}$?\n",
        "* $W = k*n$ matrix k units\n",
        "* $\\mathbf{\\hat{y}} = F(Wx +b) =[y_1, y_2,...,y_k]^T $   \n",
        "*KLAUSUR*\n",
        "*Task*: Display the parameters of the model.\n",
        "\n",
        "\n",
        "\\begin{bmatrix}\n",
        "              1 & x_1^1 & ...x_n^1\\\\\n",
        "              1 & x_1^2 & ...x_n^2\\\\\n",
        "              ... & ... & ...\\\\\n",
        "              1 & x_1^m & ... x_n^m\\\\\n",
        "\\end{bmatrix}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C8KHWXYNgfig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048f7762-73bb-4c18-9e9c-5b180e102ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear.weight: tensor([[-1.8722]])\n",
            "linear.bias: tensor([9.4385])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(f'{name}: {param.data}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv13s12hgfig"
      },
      "source": [
        "*Task*: Compare the results with the linear regression from exercise 2. Implement another neural network with 3 layers, where the first layer should have 10 neurons, the second 5 neurons and the third 1 neuron. In addition, use the `ReLU()` (recitified Linear Unit) function as the activation function of the first layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J7XIxsWogfig"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Class for our linear model. Here we have to overwrite some of the generic funcionalities of the nn.Module class of PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"This is basically how we define our model architecture.\n",
        "        This function will be automatically called when we initialize an instance of our LinearModel class.\n",
        "        \"\"\"\n",
        "        # execute initialization code for nn.Module()\n",
        "        super(MLP, self).__init__()\n",
        "        # TODO: define the building blocks for our network architecture\n",
        "        self.linear1 = nn.Linear(1,          # number of inputs into the layer\n",
        "                                10,          # number of units in the layer\n",
        "                                bias=True)  # nn.linear() has a bias term by default\n",
        "\n",
        "        self.linear2 = nn.Linear(10,          # number of inputs into the layer\n",
        "                                5,          # number of units in the layer\n",
        "                                bias=True)  # nn.linear() has a bias term by default\n",
        "\n",
        "        self.linear3 = nn.Linear(5,          # number of inputs into the layer\n",
        "                                1,          # number of units in the layer\n",
        "                                bias=True)  # nn.linear() has a bias term by default\n",
        "\n",
        "        # TODO: define the activation functions for our network\n",
        "        self.relu = nn.ReLU()               # ReLU function\n",
        "        self.identity = nn.Identity()           # Identity function\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
        "        \"\"\"With this method we define how the forward pass of the model is computed.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input to our network.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output of the network.\n",
        "        \"\"\"\n",
        "        x = self.linear1(x)    # feed input x to our first linear layer\n",
        "        x = self.relu(x)    # compute ReLU on the ouput of our linear layer 1\n",
        "        x = self.linear2(x)    # feed x to our second linear layer\n",
        "        x = self.relu(x)    # compute identity on the ouput of our linear layer 2\n",
        "        x = self.linear3(x)    # feed x to our third linear layer\n",
        "        x =  self.identity(x)    # compute identity on the ouput of our linear layer 3\n",
        "        return x\n",
        "\n",
        "# initialize the model\n",
        "model = MLP()\n",
        "\n",
        "# define the loss function and optimizer for our network\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGdlfY4fgfih"
      },
      "source": [
        "Now train your new model! Display the parameters of the model and interpret them! What are the dimensions of the individual parameters of your network?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BsYjB6pCgfih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee16373b-6b68-4d4e-95fc-836d11a94c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 1/5: 100%|██████████| 25/25 [00:00<00:00, 199.83epoch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 25 epochs: 4.141355514526367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 2/5: 100%|██████████| 25/25 [00:00<00:00, 194.40epoch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 50 epochs: 4.152929306030273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 3/5: 100%|██████████| 25/25 [00:00<00:00, 128.44epoch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 75 epochs: 4.152390003204346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 4/5: 100%|██████████| 25/25 [00:00<00:00, 297.06epoch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 100 epochs: 4.148686408996582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 5/5: 100%|██████████| 25/25 [00:00<00:00, 430.95epoch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss after 125 epochs: 4.144994735717773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "X = torch.tensor(df.SepalWidth.values, dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor(df.PetalLength.values, dtype=torch.float32).view(-1, 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
        "\n",
        "iterations = 5\n",
        "epochs = 25\n",
        "num_epochs = 0\n",
        "\n",
        "for i in range(iterations):\n",
        "    with tqdm(total=epochs, desc=f'Iteration {i+1}/{5}', unit='epoch') as pbar:\n",
        "        for epoch in range (epochs):\n",
        "            train_linear_model(model, X_train, y_train, criterion, optimizer)\n",
        "            test_loss = evaluate_linear_model(model, X_test, y_test, criterion)\n",
        "            pbar.update(1)\n",
        "        num_epochs = num_epochs+epochs\n",
        "        print(f'\\nLoss after {num_epochs} epochs: {test_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ki_RgrU2gfih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce131e43-fd3e-497e-9c76-780140d513c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear1.weight: tensor([[ 0.9242],\n",
            "        [-0.7345],\n",
            "        [ 0.8187],\n",
            "        [ 0.1533],\n",
            "        [-0.0849],\n",
            "        [ 0.1012],\n",
            "        [ 0.6942],\n",
            "        [-0.9916],\n",
            "        [-0.8407],\n",
            "        [ 1.1007]])\n",
            "linear1.bias: tensor([ 0.2010, -0.3640,  0.5897,  0.8945, -0.8068, -0.8127,  0.6193, -0.9725,\n",
            "        -0.5255,  0.0542])\n",
            "linear2.weight: tensor([[-0.0091, -0.0985,  0.1333, -0.3442,  0.2090,  0.0814, -0.0436,  0.1068,\n",
            "          0.1633,  0.0403],\n",
            "        [ 0.6790, -0.0046,  0.5983,  0.3062, -0.1705, -0.0454,  0.4906, -0.2882,\n",
            "          0.2402,  0.5087],\n",
            "        [ 0.0695, -0.2384, -0.2948, -0.0587,  0.0665, -0.1551, -0.2556, -0.1682,\n",
            "          0.0724,  0.0822],\n",
            "        [ 0.2578, -0.2512,  0.2247, -0.1631,  0.2784, -0.1273, -0.2225,  0.2579,\n",
            "         -0.0564, -0.2591],\n",
            "        [-0.2330, -0.2989,  0.2117, -0.0733, -0.1431, -0.0815, -0.1492,  0.0847,\n",
            "         -0.0137,  0.1147]])\n",
            "linear2.bias: tensor([-0.3402,  0.7645, -0.2807, -0.0897, -0.1791])\n",
            "linear3.weight: tensor([[-0.1550,  0.4306, -0.1091, -0.2055,  0.3894]])\n",
            "linear3.bias: tensor([0.2165])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(f'{name}: {param.data}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVBwmwAcgfih"
      },
      "source": [
        "## Neural networks for classification\n",
        "Next, let's look at a much more complicated data set: MNIST contains images of size 28*28 of handwritten digits. The task is to develop a classifier for these digits.    \n",
        "\n",
        "First, we load the training and test data sets. Both data sets are then loaded into so-called data loaders.    \n",
        "\n",
        "*Task*: What is a DataLoader used for? Find out for yourself how to load the data into the `DataLoader()` class and set the batch size of `train_loader` to 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pka9X20kgfih"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_data = datasets.MNIST(root=r'data', train=True, download=True,\n",
        "                                               transform=transform)\n",
        "test_data = datasets.MNIST(root=r'data', train=False, download=True,\n",
        "                                               transform=transform)\n",
        "\n",
        "# initialize dataLoader for train set\n",
        "train_loader = 'TODO'\n",
        "\n",
        "# initialize dataLoader for test set\n",
        "test_loader = 'TODO'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6WCcQkGgfih"
      },
      "source": [
        "### Attention!\n",
        "The following tasks require research on your part. Find out for yourself how you can solve the individual tasks!\n",
        "\n",
        "*Task a)*: Access the data point with the index 42 in the `train_loader`. What can you tell me about the general data structure of a data point in the `train_loader`?\n",
        "\n",
        "*Task b)*: How do you access the features $X$ and labels/targets $y$ of the individual data points? What data types and dimensions do $X$ and $y$ have? Display the label for index 42! Which data format and which dimensions do the features have?    \n",
        "\n",
        "*Task c)*: What is the data format of a batch in `train_loader`? What dimensions do the features and labels of a batch have?    \n",
        "(Hint: Use `next(iter(train_loader))` to access the first batch in `train_loader`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LarFq0htgfih"
      },
      "outputs": [],
      "source": [
        "# Task a):\n",
        "datapoint_type = 'TODO'                                                             # get type for datapoint at index 42\n",
        "print(f'The datastructure of index 42 is {datapoint_type}.\\n')\n",
        "\n",
        "# Task b):\n",
        "y_label = 'TODO'                                                                    # get label y\n",
        "print(f'Index 42 contains the number {y_label}.')\n",
        "\n",
        "x_type = 'TODO'                                                                     # get type of x\n",
        "x_dim = 'TODO'                                                                      # get dimensions of x\n",
        "print(f'The features have the dataformat {x_type} and the shape {x_dim}.\\n')\n",
        "\n",
        "# Task c):\n",
        "batch = 'TODO'                                                                      # get first batch\n",
        "batch_type = 'TODO'                                                                 # get type of batch\n",
        "print(f'A batch has the data type {batch_type}.')\n",
        "\n",
        "batch_X_dim = 'TODO'                                                                # get dimensions of X\n",
        "batch_y_dim = 'TODO'                                                                # get dimensions of y\n",
        "print(f'The features X of a batch have the dimension {batch_X_dim}.')\n",
        "print(f'The labels y of a batch have the dimension {batch_y_dim}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2ilw1FAgfii"
      },
      "source": [
        "*Task*: Create a very simple network with only one linear layer, which has 10 neurons and uses the identity as activation function. Make sure you specify the correct number of inputs for the linear layer (note: we use an image with 28x28 pixels as input).    \n",
        "\n",
        "*Task*: Which error function should we choose for the problem at hand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsgx5cwKgfii"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MNISTNet(nn.Module):\n",
        "    \"\"\"Class for our linear model. Here we have to overwrite some of the generic funcionalities of the nn.Module class of PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"This is basically how we define our model architecture.\n",
        "        This function will be automatically called when we initialize an instance of our LinearModel class.\n",
        "        \"\"\"\n",
        "        # execute initialization code for nn.Module()\n",
        "        super(MNISTNet, self).__init__()\n",
        "        # TODO: Building blocks for our network architecture\n",
        "        self.linear = 'TODO'\n",
        "\n",
        "        # TODO: define the activation functions for our network\n",
        "        self.identity = 'TODO'         # Identity function\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
        "        \"\"\"With this method we define how the forward pass of the model is computed.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input to our network.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output of the network.\n",
        "        \"\"\"\n",
        "        x = 'TODO'     # feed input x to our first linear layer\n",
        "        x = 'TODO'    # compute identity on the ouput of our linear layer 3\n",
        "        return x\n",
        "\n",
        "# initialize the model\n",
        "model = MNISTNet()\n",
        "\n",
        "# define the loss function and optimizer for our network\n",
        "criterion = 'TODO'\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, eps=1e-07) # Adam optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNhIRGDugfii"
      },
      "source": [
        "As you may have noticed, the dimensions of our features in $X$ (1x28x28) do not match the dimensions of the input (784) of our network that we have just defined. What we can do now is to transform the features of each data point into a tensor with only one dimension.    \n",
        "\n",
        "*Task*: Find out how you can transform a tensor with the dimensions nx1x28x28 into a tensor with the dimensions nx784, where n stands for the size or the number of elements of a batch. Insert your solution at the appropriate place in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apgr4_9ygfii"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model: nn.Module, dataloader: torch.utils.data.DataLoader, criterion:Callable, optimizer: Callable):\n",
        "    \"\"\"This is the function with which we can train our model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model we want to train.\n",
        "        dataloader(torch.utils.data.DataLoader): The dataloader which contains all the data.\n",
        "        criterion (Callable): Our loss function.\n",
        "        optimizer (Callable): The optimizer we want to use to train our model.\n",
        "    \"\"\"\n",
        "    with tqdm(total=len(dataloader), desc=f'\\t Training: ', unit=' batches') as pbar:\n",
        "        for X, y in dataloader:\n",
        "            X = 'TODO'\n",
        "            # compute predictions for all inputs\n",
        "            y_pred = model(X)\n",
        "            # compute average loss over all precictions\n",
        "            loss = criterion(y_pred, y)\n",
        "            # reset the optimizer\n",
        "            optimizer.zero_grad()\n",
        "            # compute the gradients\n",
        "            loss.backward()\n",
        "            # optimize the network using the computet gradients\n",
        "            optimizer.step()\n",
        "            pbar.update(1)\n",
        "\n",
        "def evaluate_model(model: nn.Module, dataloader: torch.utils.data.DataLoader, criterion: Callable) -> Tuple[float, float]:\n",
        "    \"\"\"This is the function with which we can evaluate our model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model we want to train.\n",
        "        dataloader(torch.utils.data.DataLoader): The dataloader which contains all the data.\n",
        "        criterion (Callable): Our loss function.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: Average loss and accuracy on test data.\n",
        "    \"\"\"\n",
        "    with tqdm(total=len(dataloader), desc=f'\\t Test: ', unit=' batches') as pbar:\n",
        "        # compute no gradients for all inputs -> better performance\n",
        "        with torch.no_grad():\n",
        "            loss = 0\n",
        "            correct_predictions = 0\n",
        "            total_samples = 0\n",
        "            for X, y in dataloader:\n",
        "                X = 'TODO'\n",
        "                # compute predictions for all inputs\n",
        "                y_pred = model(X)\n",
        "                # compute accumulated loss over all precictions\n",
        "                l = criterion(y_pred, y).item()\n",
        "                loss += l\n",
        "                # compute accumulated accuracy\n",
        "                _, predicted = torch.max(y_pred, 1)\n",
        "                correct_predictions += (predicted == y).sum().item()\n",
        "                total_samples += y.size(0)\n",
        "                pbar.update(1)\n",
        "    # compute avg loss and accuracy\n",
        "    loss = loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzE_uFtqgfii"
      },
      "source": [
        "Now train your model. What accuracy will you achieve in the end?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvAaAOm9gfii"
      },
      "outputs": [],
      "source": [
        "# set the number of epochs and iterations\n",
        "epochs = 1\n",
        "num_epochs = 0\n",
        "\n",
        "# execute the training and evluation\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}:')\n",
        "    train_model(model, train_loader, criterion, optimizer)\n",
        "    # evaluation of the neural network\n",
        "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
        "    num_epochs = num_epochs+epochs\n",
        "    print(f'\\t Loss: {test_loss}')\n",
        "    print(f'\\t Accuracy: {test_accuracy}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}